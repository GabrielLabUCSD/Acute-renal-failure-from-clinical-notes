{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8vBQp-emPG2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gbKalIOkjOM"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36V583TrI-lS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQJQKRuvNZXl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WblnuVQhVgi1"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t-RxQ8qYmxc"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import openai\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT9k6Mul46xR"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vBgwTjYR6H_"
      },
      "outputs": [],
      "source": [
        "def calculate_values(result, model):\n",
        "\n",
        "  precision = result['tp'] / (result['tp'] + result['fp'])\n",
        "  recall = result['tp'] / (result['tp'] + result['fn'])\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = (result['tp'] + result['tn']) / (result['tp'] + result['tn'] + result['fp'] + result['fn'])\n",
        "\n",
        "  # Calculate Specificity\n",
        "  specificity = result['tn'] / (result['tn'] + result['fp'])\n",
        "\n",
        "  # Calculate AUC-ROC\n",
        "  auc_roc = result['auroc']\n",
        "\n",
        "  # Calculate ROC\n",
        "  roc = {\n",
        "      'fpr': result['fp'] / (result['fp'] + result['tn']),\n",
        "      'tpr': recall\n",
        "  }\n",
        "\n",
        "  # Print the calculated metrics\n",
        "  print(\"Precision:\",model, \": \", precision)\n",
        "  print(\"Recall:\",model, \": \", recall)\n",
        "  print(\"F1 Score:\", model, \": \", f1)\n",
        "  print(\"Accuracy:\", model, \": \", accuracy)\n",
        "  print(\"Sensitivity:\", model, \": \", recall)\n",
        "  print(\"Specificity:\", model, \": \", specificity)\n",
        "  print(\"AUC-ROC:\", model, \": \", auc_roc)\n",
        "  print(\"ROC:\", model, \": \", roc)\n",
        "\n",
        "  return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkbUZn--R6qY"
      },
      "outputs": [],
      "source": [
        "def draw_plots(predicted_probabilities, true_labels, model):\n",
        "\n",
        "  auc_score = roc_auc_score(true_labels, predicted_probabilities)\n",
        "  fpr, tpr, _ = roc_curve(true_labels, predicted_probabilities)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, label=f\"auroc = {auc_score:.2f}\")\n",
        "  plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classification\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Rceiver Operating Characteristic Curve for ' + model)\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.show()\n",
        "\n",
        "  print(\" \")\n",
        "\n",
        "  average_precision = average_precision_score(true_labels, predicted_probabilities)\n",
        "  precision, recall, _ = precision_recall_curve(true_labels, predicted_probabilities)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(recall, precision, label=f\"auprc = {average_precision:.2f}\")\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.title('Precision-Recall Curve for ' + model)\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS6h8HoxwYG2"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "\n",
        "def draw_roc_curve(predicted_probabilities, true_labels, model_name):\n",
        "    auc_score = roc_auc_score(true_labels, predicted_probabilities)\n",
        "    fpr, tpr, _ = roc_curve(true_labels, predicted_probabilities)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.2f})\")\n",
        "\n",
        "def draw_precision_recall_curve(predicted_probabilities, true_labels, model_name):\n",
        "    average_precision = average_precision_score(true_labels, predicted_probabilities)\n",
        "    precision, recall, _ = precision_recall_curve(true_labels, predicted_probabilities)\n",
        "\n",
        "    plt.plot(recall, precision, label=f\"{model_name} (AP = {average_precision:.2f})\")\n",
        "\n",
        "def draw_plots_multiple(predicted_probabilities_list, true_labels_list, model_names):\n",
        "    plt.figure()\n",
        "\n",
        "    for i, predicted_probabilities in enumerate(predicted_probabilities_list):\n",
        "        model_name = model_names[i]\n",
        "        true_labels = true_labels_list[i]\n",
        "        draw_roc_curve(predicted_probabilities, true_labels, model_name)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic Curve')\n",
        "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "\n",
        "    print(\" \")\n",
        "\n",
        "    for i, predicted_probabilities in enumerate(predicted_probabilities_list):\n",
        "        model_name = model_names[i]\n",
        "        true_labels = true_labels_list[i]\n",
        "        draw_precision_recall_curve(predicted_probabilities, true_labels, model_name)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ANvSmIfVZUF"
      },
      "source": [
        "## Mimic Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro6Sfc6lHUPz"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEddxTJ1QEJ2"
      },
      "outputs": [],
      "source": [
        "df_renal = pd.read_csv('data_renal_failure.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsdXKBqNQLXt"
      },
      "outputs": [],
      "source": [
        "df_renal.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS2BMYH-QNR8"
      },
      "outputs": [],
      "source": [
        "df_renal['renal_failure'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcfdYnXmUNcm"
      },
      "outputs": [],
      "source": [
        "df_renal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knyzFKMTUNal"
      },
      "outputs": [],
      "source": [
        "df_renal = df_renal.dropna(subset=['TEXT'])\n",
        "df_renal = df_renal.dropna(subset=['renal_failure'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52JhWyKVQTgB"
      },
      "outputs": [],
      "source": [
        "df_renal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CY5YPASC9c"
      },
      "outputs": [],
      "source": [
        "for index, row in df_renal.iterrows():\n",
        "  df_renal['TEXT'][index] = df_renal['TEXT'][index].replace('\\n', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGa5pIPBVRQh"
      },
      "outputs": [],
      "source": [
        "df_renal['TEXT']= df_renal['TEXT'].str.replace(r'\\s+', ' ').str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IigU3e9S2VW"
      },
      "outputs": [],
      "source": [
        "renal_text = df_renal['TEXT'].tolist()\n",
        "renal_label = df_renal['renal_failure'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb6__A5rTr8D"
      },
      "outputs": [],
      "source": [
        "renal_label = [int(x) for x in renal_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnBuSNAMaOCw"
      },
      "outputs": [],
      "source": [
        "bhc_1 = list()\n",
        "bhc_2 = list()\n",
        "bhc_3 = list()\n",
        "no_bhc = list()\n",
        "for i in range(len(renal_text)):\n",
        "  t = renal_text[i].lower()\n",
        "  if t.find('brief summary of hospital course')!=-1: bhc_1.append(i)\n",
        "  elif t.find('brief hospital course')!=-1: bhc_2.append(i)\n",
        "  elif t.find('hospital course:')!=-1: bhc_3.append(i)\n",
        "  else: no_bhc.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfMXAdEHa4zX"
      },
      "outputs": [],
      "source": [
        "print(len(bhc_1))\n",
        "print(len(bhc_2))\n",
        "print(len(bhc_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48-9Z9crZMrC"
      },
      "outputs": [],
      "source": [
        "def get_bhc(text):\n",
        "  t = text.lower()\n",
        "\n",
        "  istart = t.find('brief hospital course')\n",
        "  offset = 22\n",
        "  if istart==-1:\n",
        "    istart = t.find('brief summary of hospital course')\n",
        "    offset = 33\n",
        "  if istart==-1:\n",
        "    istart = t.find('hospital course:')\n",
        "    offset = 16\n",
        "\n",
        "  if istart!=-1:\n",
        "    t_split = t[istart+offset:].split('.')\n",
        "    text_li = list()\n",
        "    count = 0\n",
        "    j = 0\n",
        "    while j<len(t_split) and count<=512:\n",
        "      count += len(t_split[j].split(' '))\n",
        "      text_li.append(t_split[j])\n",
        "      j+=1\n",
        "    text_li = '. '.join(text_li)\n",
        "    return text_li\n",
        "  else:\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eDlHsOyZMo8"
      },
      "outputs": [],
      "source": [
        "bhc_list = list()\n",
        "for i in range(len(renal_text)):\n",
        "  bhc_list.append(get_bhc(renal_text[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDGAID1wkHKD"
      },
      "outputs": [],
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(bhc_list, renal_label, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxltNkGOkrnZ"
      },
      "outputs": [],
      "source": [
        "train_data = {'text': train_X, 'labels': train_y}\n",
        "test_data = {'text': test_X, 'labels': test_y}\n",
        "\n",
        "df_train_data_mimic = pd.DataFrame(train_data)\n",
        "df_test_data_mimic = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWiLHQaIk_Zx"
      },
      "outputs": [],
      "source": [
        "df_test_data_mimic['labels'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95acI0J0lBcr"
      },
      "outputs": [],
      "source": [
        "df_train_data_mimic['labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPao3UHfOvgr"
      },
      "source": [
        "## Mimic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "\n",
        "n = 20"
      ],
      "metadata": {
        "id": "pTvx3STL7Ub0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWK5aBGtgwpJ"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE8MHORTlDoC"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True, max_seq_length=512)\n",
        "\n",
        "model_roberta = ClassificationModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sueMfh-WLCHL"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "roberta_mimic_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_roberta.train_model(df_train_data_mimic)\n",
        "  result, model_outputs, wrong_predictions = model_roberta.eval_model(df_test_data_mimic)\n",
        "  roberta_mimic_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Roberta')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_roberta_mimic = model_outputs[:, 1]\n",
        "    true_labels_roberta_mimic = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpaR9CKDg1lB"
      },
      "source": [
        "### BioClincal BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA_JpfCWhyle"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_Bio_ClinicalBERT= ClassificationModel(\n",
        "    \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDm5kQBuhpBv"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "biobert_mimic_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_Bio_ClinicalBERT.train_model(df_train_data_mimic)\n",
        "  result, model_outputs, wrong_predictions = model_Bio_ClinicalBERT.eval_model(df_test_data_mimic)\n",
        "  biobert_mimic_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Bert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_biobert_mimic = model_outputs[:, 1]\n",
        "    true_labels_biobert_mimic = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ustA3O_FXmY"
      },
      "source": [
        "### PubMedBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LljCF-lWFZ6Q"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_microsoft_bio_pubmed =  ClassificationModel(\n",
        "    \"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdNrRtCaFZ4N"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "pubbert_mimic_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_microsoft_bio_pubmed.train_model(df_train_data_mimic)\n",
        "  result, model_outputs, wrong_predictions = model_microsoft_bio_pubmed.eval_model(df_test_data_mimic)\n",
        "  pubbert_mimic_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Bert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_pubbert_mimic = model_outputs[:, 1]\n",
        "    true_labels_pubbert_mimic = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nc2xRJNVc5x"
      },
      "source": [
        "## GPT Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaf7-PVVmb91"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj5ENyz0YqDj"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoznmbZhYrO1"
      },
      "outputs": [],
      "source": [
        "command_pos = \"\"\"Create a patient summary with 15 sentences describing a patient's medical history who is planning to have any type of\n",
        "surgery. Also describe patient having acute renal failure complication after surgery.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgpaDPACY2g-"
      },
      "outputs": [],
      "source": [
        "command_neg = \"\"\"create a patient summary with 15 sentences describing a patient's medical history who is planning to have any type of\n",
        "surgery. Also describe patient 's postoperative course and if they had complications. Do not include acute renal failure as a complication.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdqtbkRY-gm"
      },
      "outputs": [],
      "source": [
        "def generate_data(command, file_label, n):\n",
        "  for i in range(n):\n",
        "    message = [{\"role\": \"system\", \"content\": command}]\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = message)\n",
        "    file_name = file_label + str(783 + i) + '.txt'\n",
        "    with open(file_name, \"w\") as file:\n",
        "      file.write(response.choices[0].message.content)\n",
        "    print('Done: ', 783 +  i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUEGxBJ3ZE1l"
      },
      "outputs": [],
      "source": [
        "# %cd GPT_data_Positive_15_Sentences\n",
        "\n",
        "# generate_data(command_pos, 'Positive', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsaLFcfKaIPh"
      },
      "outputs": [],
      "source": [
        "%cd GPT_data_Negative_15_Sentences\n",
        "\n",
        "generate_data(command_neg, 'Negative',117)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7LIF83EaPXH"
      },
      "outputs": [],
      "source": [
        "def read_data(directory_path, X, y, keyword):\n",
        "\n",
        "  c = list()\n",
        "  for file_path in directory_path.iterdir():\n",
        "    with open(file_path) as f:\n",
        "      data = f.read().strip()\n",
        "      data = data.replace(\"\\n\\n\", \"\")\n",
        "      data = data.replace(\"\\n\", \"\")\n",
        "\n",
        "      X.append(data)\n",
        "      if keyword == 'Positive':\n",
        "        y.append(1)\n",
        "      elif keyword == 'Negative':\n",
        "        y.append(0)\n",
        "\n",
        "      if len(data.split(\" \")) > 512:\n",
        "        c.append(len(data.split(\" \")))\n",
        "\n",
        "  print(keyword, \"Labels greater than 512: \", len(c))\n",
        "  print(c)\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyXi2qYyZpId"
      },
      "outputs": [],
      "source": [
        "# Take 567 negative and 100 positive labels for train dataset to have same ratio as that of MIMIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oltiCEdP1TMw"
      },
      "outputs": [],
      "source": [
        "def get_train_test_gpt(keyword):\n",
        "  X = list()\n",
        "  y = list()\n",
        "\n",
        "  directory_path = Path(\"GPT_data_Negative_\" + keyword + \"_Sentences\")\n",
        "  X, y = read_data(directory_path, X, y, \"Negative\")\n",
        "\n",
        "  directory_path = Path(\"GPT_data_Positive_\" + keyword + \"_Sentences\")\n",
        "  X, y = read_data(directory_path, X, y, \"Positive\")\n",
        "\n",
        "  df = pd.DataFrame({'text': X, 'labels': y})\n",
        "\n",
        "  np.random.seed(0)\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  selected_label_0 = df[df['labels'] == 0].head(567)\n",
        "  selected_label_1 = df[df['labels'] == 1]\n",
        "  selected_rows = pd.concat([selected_label_0, selected_label_1])\n",
        "\n",
        "  train_data, test_data = train_test_split(selected_rows, test_size=0.25, random_state=42)\n",
        "  train_data = train_data.reset_index(drop = True)\n",
        "  test_data = test_data.reset_index(drop = True)\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "  # return selected_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__z3eBEQGzjn"
      },
      "source": [
        "## 15 Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RC5BkEm1zbz"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = get_train_test_gpt(\"15\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syNPiaUUe9uy"
      },
      "outputs": [],
      "source": [
        "print(train_data['labels'].value_counts())\n",
        "\n",
        "print(test_data['labels'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "\n",
        "n = 20"
      ],
      "metadata": {
        "id": "DhNM6y7N7ljv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoV7ABnorvo7"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm-1WQ3tzQwy"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True, max_seq_length=512)\n",
        "\n",
        "model_roberta = ClassificationModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqwFXvsUzLp5"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "roberta_gpt15_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_roberta.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_roberta.eval_model(df_test_data_mimic)\n",
        "  roberta_gpt15_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Roberta')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_roberta_gpt15 = model_outputs[:, 1]\n",
        "    true_labels_roberta_gpt15 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LBzCOoXcAiY"
      },
      "source": [
        "### BioBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcyJ5SuCcCUJ"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_Bio_ClinicalBERT= ClassificationModel(\n",
        "    \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmq2kPCbcCSG"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "biobert_gpt15_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_Bio_ClinicalBERT.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_Bio_ClinicalBERT.eval_model(df_test_data_mimic)\n",
        "  biobert_gpt15_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_biobert_gpt15 = model_outputs[:, 1]\n",
        "    true_labels_bert_biogpt15 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE-ModWDOWch"
      },
      "source": [
        "### PubMedBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV-ila5KOX_y"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_microsoft_bio_pubmed =  ClassificationModel(\n",
        "    \"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InOMFqXZOX97"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "pubbert_gpt15_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_microsoft_bio_pubmed.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_microsoft_bio_pubmed.eval_model(df_test_data_mimic)\n",
        "  pubbert_gpt15_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_pubbert_gpt15 = model_outputs[:, 1]\n",
        "    true_labels_pubbert_gpt15 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4SvQEgtHEdI"
      },
      "source": [
        "## 30 Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_aj42f313jW"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = get_train_test_gpt(\"30\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LejrnexPZA-"
      },
      "outputs": [],
      "source": [
        "print(train_data['labels'].value_counts())\n",
        "\n",
        "print(test_data['labels'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "\n",
        "n = 20"
      ],
      "metadata": {
        "id": "P6raIvqz7vE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0VgX3l5uvP6"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgN4cXjC0CTn"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True, max_seq_length=512)\n",
        "\n",
        "model_roberta = ClassificationModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5RywcwS0CTn"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "roberta_gpt30_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_roberta.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_roberta.eval_model(df_test_data_mimic)\n",
        "  roberta_gpt30_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Roberta')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_roberta_gpt30 = model_outputs[:, 1]\n",
        "    true_labels_roberta_gpt30 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVdt1RoacLG7"
      },
      "source": [
        "### BioBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ScMPHvcMuh"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_Bio_ClinicalBERT= ClassificationModel(\n",
        "    \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X52dhcdUcMsd"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "biobert_gpt30_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_Bio_ClinicalBERT.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_Bio_ClinicalBERT.eval_model(df_test_data_mimic)\n",
        "  biobert_gpt30_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_biobert_gpt30 = model_outputs[:, 1]\n",
        "    true_labels_bioert_gpt30 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AVU6e1ENlOT"
      },
      "source": [
        "### PubMedBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRl6r0bIOKuB"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_microsoft_bio_pubmed =  ClassificationModel(\n",
        "    \"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWQldrQGONnq"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "pubbert_gpt30_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_microsoft_bio_pubmed.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_microsoft_bio_pubmed.eval_model(df_test_data_mimic)\n",
        "  pubbert_gpt30_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_pubbert_gpt30 = model_outputs[:, 1]\n",
        "    true_labels_pubbert_gpt30 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706V89YdHIiR"
      },
      "source": [
        "## 45 Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH0o9LIC15Yk"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = get_train_test_gpt(\"45\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_s-8zB0siHe"
      },
      "outputs": [],
      "source": [
        "print(train_data['labels'].value_counts())\n",
        "\n",
        "print(test_data['labels'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "\n",
        "n = 20"
      ],
      "metadata": {
        "id": "nEmzwVKy71vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE5fuSQOvXbu"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk9YuFcV0SEj"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True, max_seq_length=512)\n",
        "\n",
        "model_roberta = ClassificationModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h2533Kb0SEj"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "roberta_gpt45_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_roberta.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_roberta.eval_model(df_test_data_mimic)\n",
        "  roberta_gpt45_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'Roberta')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_roberta_gpt45 = model_outputs[:, 1]\n",
        "    true_labels_roberta_gpt45 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdMqTTZBe96j"
      },
      "source": [
        "### BioBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcAQwIuWe_uC"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_Bio_ClinicalBERT= ClassificationModel(\n",
        "    \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "991JSIPpe_rr"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "biobert_gpt45_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_Bio_ClinicalBERT.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_Bio_ClinicalBERT.eval_model(df_test_data_mimic)\n",
        "  biobert_gpt45_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_biobert_gpt45 = model_outputs[:, 1]\n",
        "    true_labels_biobert_gpt45 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRHE8yJgNJMj"
      },
      "source": [
        "### PubMedBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bGvTSuy6NMZK"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir = True)\n",
        "\n",
        "model_microsoft_bio_pubmed =  ClassificationModel(\n",
        "    \"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", args=model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yMU_gOq8NMWm"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "pubbert_gpt45_graph_list = list()\n",
        "\n",
        "for i in range(n):\n",
        "  print('Epoch: ', i)\n",
        "  model_microsoft_bio_pubmed.train_model(train_data)\n",
        "  result, model_outputs, wrong_predictions = model_microsoft_bio_pubmed.eval_model(df_test_data_mimic)\n",
        "  pubbert_gpt45_graph_list.append([model_outputs[:, 1], df_test_data_mimic['labels'].tolist()])\n",
        "  print('Result')\n",
        "  print(result)\n",
        "  evaluation_metric_f1 = calculate_values(result, 'BioBert')\n",
        "  print(\" \")\n",
        "\n",
        "  if evaluation_metric_f1>=best_f1:\n",
        "    best_f1 = evaluation_metric_f1\n",
        "    print('Stored for Epoch: ', i)\n",
        "    predicted_probabilities_pubbert_gpt45 = model_outputs[:, 1]\n",
        "    true_labels_pubbert_gpt45 = np.array(df_test_data_mimic['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhfL2U0jghpj"
      },
      "source": [
        "## Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9HkwNN0jm2"
      },
      "source": [
        "### Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCFIDs5Pgi1e"
      },
      "outputs": [],
      "source": [
        "predicted_probabilities_list = [predicted_probabilities_roberta_mimic, predicted_probabilities_roberta_gpt15, predicted_probabilities_roberta_gpt30, predicted_probabilities_roberta_gpt45]\n",
        "true_labels_list = [true_labels_roberta_mimic, true_labels_roberta_gpt15, true_labels_roberta_gpt30, true_labels_roberta_gpt45]\n",
        "model_names = [\"Mimic\", \"GPT-15 Sentences\", \"GPT-30 Sentences\", \"GPT-45 Sentences\"]\n",
        "\n",
        "draw_plots_multiple(predicted_probabilities_list, true_labels_list, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1k0kBGvCPuW"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xfdfyEa3QHc"
      },
      "outputs": [],
      "source": [
        "predicted_probabilities_list = [predicted_probabilities_bert_mimic, bert_gpt15_graph_list[-1][0], bert_gpt30_graph_list[-1][0], bert_gpt45_graph_list[-1][0]]\n",
        "true_labels_list = [true_labels_bert_mimic, bert_gpt15_graph_list[-1][1], bert_gpt30_graph_list[-1][1], bert_gpt45_graph_list[-1][1]]\n",
        "model_names = [\"Mimic\", \"GPT-15 Sentences\", \"GPT-30 Sentences\", \"GPT-45 Sentences\"]\n",
        "\n",
        "draw_plots_multiple(predicted_probabilities_list, true_labels_list, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S30HzIlqfjFA"
      },
      "source": [
        "### BioBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdKt8Iec40jB"
      },
      "outputs": [],
      "source": [
        "predicted_probabilities_list = [predicted_probabilities_biobert_mimic, predicted_probabilities_biobert_gpt15, predicted_probabilities_biobert_gpt30, predicted_probabilities_biobert_gpt45]\n",
        "true_labels_list = [true_labels_biobert_mimic, true_labels_bert_biogpt15, true_labels_bioert_gpt30, true_labels_biobert_gpt45]\n",
        "model_names = [\"Mimic\", \"GPT-15 Sentences\", \"GPT-30 Sentences\", \"GPT-45 Sentences\"]\n",
        "\n",
        "draw_plots_multiple(predicted_probabilities_list, true_labels_list, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guqq6gHCP7Jo"
      },
      "source": [
        "### PubMedBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6koaPc3DoUdb"
      },
      "outputs": [],
      "source": [
        "predicted_probabilities_list = [predicted_probabilities_pubbert_mimic, predicted_probabilities_pubbert_gpt15, predicted_probabilities_pubbert_gpt30, predicted_probabilities_bert_gpt45]\n",
        "true_labels_list = [true_labels_pubbert_mimic, true_labels_pubbert_gpt15, true_labels_pubbert_gpt30, true_labels_pubbert_gpt45]\n",
        "model_names = [\"Mimic\", \"GPT-15 Sentences\", \"GPT-30 Sentences\", \"GPT-45 Sentences\"]\n",
        "\n",
        "draw_plots_multiple(predicted_probabilities_list, true_labels_list, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AK7g7Jdl0mQ"
      },
      "source": [
        "## Data Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0989qA8n8jl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import statistics\n",
        "nltk.download('stopwords')\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzfCEgR6l38W"
      },
      "outputs": [],
      "source": [
        "# bhc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWmXyv4umQiQ"
      },
      "outputs": [],
      "source": [
        "# Make modificaitons in the above function to just return list of sentences\n",
        "\n",
        "gpt15 = get_train_test_gpt(\"15\")\n",
        "gpt30 = get_train_test_gpt(\"30\")\n",
        "gpt45 = get_train_test_gpt(\"45\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWKYMR39mma1"
      },
      "outputs": [],
      "source": [
        "gpt15 = gpt15['text'].tolist()\n",
        "gpt30 = gpt30['text'].tolist()\n",
        "gpt45 = gpt45['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVL1QHE0nXPC"
      },
      "outputs": [],
      "source": [
        "def calculate_average_sentence_count(note_list):\n",
        "    total_sentence_count = list()\n",
        "\n",
        "    for note_content in note_list:\n",
        "        sentences = sent_tokenize(note_content)\n",
        "        c = 0\n",
        "        for i in range(len(sentences)):\n",
        "          if len(sentences[i]) >=5:\n",
        "            c+=1\n",
        "        total_sentence_count.append(c)\n",
        "\n",
        "    # return statistics.mean(total_sentence_count)\n",
        "    return total_sentence_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(data):\n",
        "    # Calculate the first and third quartiles (Q1 and Q3)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q3 = np.percentile(data, 75)\n",
        "\n",
        "    # Calculate the interquartile range (IQR)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Define the lower and upper bounds for outliers\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    # Remove outliers\n",
        "    filtered_data = [x for x in data if lower_bound <= x <= upper_bound]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "def calculate_average(data):\n",
        "    # Calculate the average of the filtered data\n",
        "    if len(data) > 0:\n",
        "        average = sum(data) / len(data)\n",
        "        return average\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "average_without_outliers_mimic = calculate_average(remove_outliers(calculate_average_sentence_count(bhc_list)))\n",
        "average_without_outliers_gpt15 = calculate_average(remove_outliers(calculate_average_sentence_count(gpt15)))\n",
        "average_without_outliers_gpt30 = calculate_average(remove_outliers(calculate_average_sentence_count(gpt30)))\n",
        "average_without_outliers_gpt45 = calculate_average(remove_outliers(calculate_average_sentence_count(gpt45)))\n",
        "\n",
        "print(\"Average without outliers Mimic:\", average_without_outliers_mimic)\n",
        "print(\"Average without outliers GPT-15:\", average_without_outliers_gpt15)\n",
        "print(\"Average without outliers GPT-30:\", average_without_outliers_gpt30)\n",
        "print(\"Average without outliers GPT-45:\", average_without_outliers_gpt45)"
      ],
      "metadata": {
        "id": "1lm_tI9OlbqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38szElk3JTJ8"
      },
      "outputs": [],
      "source": [
        "std_deviation_mimic = statistics.stdev(remove_outliers(calculate_average_sentence_count(bhc_list)))\n",
        "std_deviation_gpt15 = statistics.stdev(remove_outliers(calculate_average_sentence_count(gpt15)))\n",
        "std_deviation_gpt30 = statistics.stdev(remove_outliers(calculate_average_sentence_count(gpt30)))\n",
        "std_deviation_gpt45 = statistics.stdev(remove_outliers(calculate_average_sentence_count(gpt45)))\n",
        "\n",
        "print(f'The standard deviation of the numbers is: {std_deviation_mimic:.2f}')\n",
        "print(f'The standard deviation of the numbers is: {std_deviation_gpt15:.2f}')\n",
        "print(f'The standard deviation of the numbers is: {std_deviation_gpt30:.2f}')\n",
        "print(f'The standard deviation of the numbers is: {std_deviation_gpt45:.2f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mPao3UHfOvgr",
        "fWK5aBGtgwpJ",
        "cpaR9CKDg1lB",
        "_ustA3O_FXmY",
        "QoV7ABnorvo7",
        "8LBzCOoXcAiY",
        "lE-ModWDOWch",
        "i0VgX3l5uvP6",
        "HVdt1RoacLG7",
        "0AVU6e1ENlOT",
        "XE5fuSQOvXbu",
        "YhfL2U0jghpj",
        "qD9HkwNN0jm2",
        "U1k0kBGvCPuW",
        "S30HzIlqfjFA",
        "Guqq6gHCP7Jo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}